{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f01520",
   "metadata": {},
   "source": [
    "# Introduction To Natural Language Processing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0012ee0",
   "metadata": {},
   "source": [
    "Language is the most important tool for communication invented by human civilization. It is either spoken or written, consisting of the use of words in a structured and conventional way. Language helps us share our thoughts, and understand others.\n",
    "\n",
    "Natural Language Processing is a form of artificial intelligence, all about trying to analyzie and understand either written or spoken language and the context that it's being used in. The ultimate objective of NLP is to read, decipher, understand, and mamke sense of human languages in a manner that is valuable.\n",
    "\n",
    "\n",
    "Wikipedia defines NLP as \"a subfield of AI concerned with the interactions between computers and human natural languages, in particular, how to program computers to process and analyze large amounts of natural language data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c386f35",
   "metadata": {},
   "source": [
    "# Data Preprocessing - NLP"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fcbea713",
   "metadata": {},
   "source": [
    "In this session, I will show you the ways of cleaning the text for the preparation of the dataset in NLP. I will be using the built-in python function and also introduce the NLTK library in the next session. Taking the data preprocessing in NLP, I encounter the steps like splitting the documents into sentences, words and there are various ways to split the texts. Here I will go through some of the ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc72784",
   "metadata": {},
   "source": [
    "# 1 Split by white spaces"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01f09477",
   "metadata": {},
   "source": [
    "Splitting by white spaces refers to the splitting of documents or texts by word. Applying split() with no input parameters calls the function to split text by looking at white spaces only. It doesn't take account of any apostrphe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66f4b6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albert', 'Einsten', 'is', 'one', 'of', 'the', 'most', 'brilliant', 'scientist', \"who's\", 'ever', 'lived']\n"
     ]
    }
   ],
   "source": [
    "#Eg -> Look how who's is split\n",
    "text = \"Albert Einsten is one of the most brilliant scientist who's ever lived\"\n",
    "#Split into words by white space\n",
    "words = text.split()\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ba4d46",
   "metadata": {},
   "source": [
    "# 2 Split by words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c66576b9",
   "metadata": {},
   "source": [
    "It is already clear by title about its function. Do you know the difference between split by words and split by\n",
    "white space? Notice the difference in “who’s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da994fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albert', 'Einsten', 'is', 'one', 'of', 'the', 'most', 'brilliant', 'scientist', 'who', 's', 'ever', 'lived']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#split based on words only\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3576b",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7642ac9a",
   "metadata": {},
   "source": [
    "In NLP, we convert all uppercase character to lowercase. We don't recommend using this step in every dataset preprocessing. Normalizing the words can change the entire meaning.\n",
    "\n",
    "Eg -: Orange is a French Telecom company whereas orange's fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4fbb763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['albert', 'einsten', 'is', 'one', 'of', 'the', 'most', 'brilliant', 'scientist', 'who', 's', 'ever', 'lived']\n"
     ]
    }
   ],
   "source": [
    "#Split based on words only\n",
    "words = re.split(r'\\W+', text)\n",
    "\n",
    "#convert to lowercase\n",
    "#Using the list comprehension\n",
    "words = [word.lower() for word in words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e9b1c8",
   "metadata": {},
   "source": [
    "# Natural Language ToolKit(NLTK)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "534b22d8",
   "metadata": {},
   "source": [
    "NLTK, Natural Language Toolkit, is an open-source Python platform to work on Natural Language Processing. The Library requires Python 3.5, 3.6, 3.7 or 3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb568d",
   "metadata": {},
   "source": [
    "# 1 Split By Sentence"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b04f01c",
   "metadata": {},
   "source": [
    "The tokenizer divides a text into a list of sentences by using an un-supervised algorithm to build a model for abbreviation words, c ollocations and words that start sentences. We sould train on a large collection of plain text in the target language before using them.\n",
    "\n",
    "The NLTK data package includes a pre-trained Punkt tokenizer for the English Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac24388d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einsten is one of the most brilliant scientist who's ever lived\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "#split into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64dc91",
   "metadata": {},
   "source": [
    "# Split by words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97ad44d6",
   "metadata": {},
   "source": [
    "Make sure you check out the output and spot the difference in who's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80d94788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albert', 'Einsten', 'is', 'one', 'of', 'the', 'most', 'brilliant', 'scientist', 'who', \"'s\", 'ever', 'lived']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Split into words\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a43ed",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fd19b36",
   "metadata": {},
   "source": [
    "Python includes the built-in function isalpha() that can be used in order to determine whether or not the scanned word is alphabetical or else (numerical, punctuation, special characters, etc). Make sure you check the output and spot the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6180684f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albert', 'Einsten', 'is', 'one', 'of', 'the', 'most', 'brilliant', 'scientist', 'who', 'ever', 'lived']\n"
     ]
    }
   ],
   "source": [
    "#Split into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "#Remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e604422",
   "metadata": {},
   "source": [
    "# 4 Remove stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1ed1e3f",
   "metadata": {},
   "source": [
    "Stopwords are words that do not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. The most common are short function words such as the, is, at, which, and on etc.\n",
    "\n",
    "In this case, removing stopwords can cause problems when searching for phrases that include them, particulary in names such as 'The Who' or 'Take That'.\n",
    "\n",
    "Including the word 'not' as a stopword also changes the entire meaning if removed (try 'this code is not good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15474d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Let's list all the stopwords for NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffcaeb9a",
   "metadata": {},
   "source": [
    "As you can see, the stopwords are all lowercase and don't have punctuation. If we're to compare them with our tokens, we need to make sure that our text is prepared the same way.\n",
    "\n",
    "This cell recaps all that we have previously learned in the collab: tokenizing, lower casing, and checking for alphabetic words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31301dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['albert', 'einsten', 'one', 'brilliant', 'scientist', 'ever', 'lived']\n"
     ]
    }
   ],
   "source": [
    "#Clean our text\n",
    "#Split into words\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "#convert to lower\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "#removes all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "#filter out stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word = [w for w in words if not w in stop_words]\n",
    "print(word[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLTK_KERNEL",
   "language": "python",
   "name": "nltk_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
